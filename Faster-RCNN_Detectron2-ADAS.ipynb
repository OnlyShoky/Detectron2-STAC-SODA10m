{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3bda5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.0 ; cuda:  2.0.1\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26944dd3",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22e63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import torch\n",
    "from source.data_utils import *\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ed132",
   "metadata": {},
   "source": [
    "## Register the SODA10M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0650e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:47:01 d2.data.datasets.coco]: \u001b[0mLoaded 5000 images in COCO format from SSLAD-2D/labeled/annotations/instance_val.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namespace(name='soda10m_val_limited',\n",
       "          thing_classes=['Pedestrian',\n",
       "                         'Cyclist',\n",
       "                         'Car',\n",
       "                         'Truck',\n",
       "                         'Tram',\n",
       "                         'Tricycle'],\n",
       "          thing_dataset_id_to_contiguous_id={1: 0,\n",
       "                                             2: 1,\n",
       "                                             3: 2,\n",
       "                                             4: 3,\n",
       "                                             5: 4,\n",
       "                                             6: 5})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "\n",
    "# Register the COCO dataset from the custom path\n",
    "# Assuming you have \"instances_train.json\" and \"instances_val.json\" in the directory\n",
    "# Update these json file names if yours are different\n",
    "register_coco_instances(\"soda10m_train\", {}, \n",
    "                        \"SSLAD-2D/labeled/annotations/instance_train.json\", \n",
    "                        \"SSLAD-2D/labeled/train\")\n",
    "register_coco_instances(\"soda10m_val\", {}, \n",
    "                        \"SSLAD-2D/labeled/annotations/instance_val.json\", \n",
    "                        \"SSLAD-2D/labeled/val\")\n",
    "register_coco_instances(\"soda10m_test\", {}, \n",
    "                        \"SSLAD-2D/labeled/annotations/instance_test.json\", \n",
    "                        \"SSLAD-2D/labeled/test\")\n",
    "# Call this function with the path to your original and new subset JSON files\n",
    "#create_val_subset(\"SSLAD-2D/labeled/annotations/instance_val.json\",\"SSLAD-2D/labeled/annotations/instance_val_subset.json\",1000)\n",
    "\n",
    "#Subset of 1000 images\n",
    "register_coco_instances(\"soda10m_val_subset\", {}, \n",
    "                        \"SSLAD-2D/labeled/annotations/instance_val_subset.json\", \n",
    "                        \"SSLAD-2D/labeled/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014291a5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b0f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        # This method returns the evaluator instance\n",
    "        return cls.build_evaluator(cfg, dataset_name, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:49:29 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:49:29 d2.data.datasets.coco]: \u001b[0mLoaded 5000 images in COCO format from SSLAD-2D/labeled/annotations/instance_train.json\n",
      "\u001b[32m[11/30 10:49:29 d2.data.build]: \u001b[0mRemoved 33 images with no usable annotations. 4967 images left.\n",
      "\u001b[32m[11/30 10:49:29 d2.data.build]: \u001b[0mDistribution of instances among all 6 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "| Pedestrian | 4901         |  Cyclist   | 6548         |    Car     | 23456        |\n",
      "|   Truck    | 4297         |    Tram    | 1681         |  Tricycle  | 227          |\n",
      "|            |              |            |              |            |              |\n",
      "|   total    | 41110        |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[11/30 10:49:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[11/30 10:49:29 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[11/30 10:49:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/30 10:49:29 d2.data.common]: \u001b[0mSerializing 4967 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/30 10:49:29 d2.data.common]: \u001b[0mSerialized dataset takes 2.28 MiB\n",
      "\u001b[32m[11/30 10:49:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[11/30 10:49:29 d2.data.datasets.coco]: \u001b[0mLoaded 1000 images in COCO format from SSLAD-2D/labeled/annotations/instance_val_subset.json\n",
      "\u001b[32m[11/30 10:49:29 d2.data.build]: \u001b[0mDistribution of instances among all 6 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "| Pedestrian | 623          |  Cyclist   | 596          |    Car     | 4087         |\n",
      "|   Truck    | 1923         |    Tram    | 266          |  Tricycle  | 20           |\n",
      "|            |              |            |              |            |              |\n",
      "|   total    | 7515         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[11/30 10:49:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/30 10:49:29 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/30 10:49:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.43 MiB\n",
      "\u001b[32m[11/30 10:49:29 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (7, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (24, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (24,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:49:30 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melmourabitagharbi\\.conda\\envs\\p38_env\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:49:47 d2.utils.events]: \u001b[0m eta: 0:10:28  iter: 19  total_loss: 2.486  loss_cls: 1.93  loss_box_reg: 0.5242  loss_rpn_cls: 0.03106  loss_rpn_loc: 0.05796    time: 0.6664  last_time: 0.6715  data_time: 0.0976  last_data_time: 0.0009   lr: 4.9952e-05  max_mem: 2586M\n",
      "\u001b[32m[11/30 10:50:12 d2.utils.events]: \u001b[0m eta: 0:11:15  iter: 39  total_loss: 1.724  loss_cls: 1.044  loss_box_reg: 0.5497  loss_rpn_cls: 0.03532  loss_rpn_loc: 0.07961    time: 0.8308  last_time: 0.6776  data_time: 0.0011  last_data_time: 0.0009   lr: 9.9902e-05  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:50:27 d2.utils.events]: \u001b[0m eta: 0:11:13  iter: 59  total_loss: 1.345  loss_cls: 0.6012  loss_box_reg: 0.617  loss_rpn_cls: 0.03407  loss_rpn_loc: 0.06404    time: 0.8053  last_time: 0.7119  data_time: 0.0012  last_data_time: 0.0012   lr: 0.00014985  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:50:43 d2.utils.events]: \u001b[0m eta: 0:11:31  iter: 79  total_loss: 1.366  loss_cls: 0.542  loss_box_reg: 0.5904  loss_rpn_cls: 0.03432  loss_rpn_loc: 0.08925    time: 0.7978  last_time: 0.8252  data_time: 0.0012  last_data_time: 0.0017   lr: 0.0001998  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:50:58 d2.data.datasets.coco]: \u001b[0mLoaded 1000 images in COCO format from SSLAD-2D/labeled/annotations/instance_val_subset.json\n",
      "\u001b[32m[11/30 10:50:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/30 10:50:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/30 10:50:58 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/30 10:50:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.43 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/30 10:50:58 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[11/30 10:50:58 d2.evaluation.evaluator]: \u001b[0mStart inference on 1000 batches\n",
      "\u001b[32m[11/30 10:51:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/1000. Dataloading: 0.0005 s/iter. Inference: 0.1006 s/iter. Eval: 0.0003 s/iter. Total: 0.1014 s/iter. ETA=0:01:40\n",
      "\u001b[32m[11/30 10:51:06 d2.evaluation.evaluator]: \u001b[0mInference done 61/1000. Dataloading: 0.0006 s/iter. Inference: 0.1006 s/iter. Eval: 0.0002 s/iter. Total: 0.1015 s/iter. ETA=0:01:35\n",
      "\u001b[32m[11/30 10:51:11 d2.evaluation.evaluator]: \u001b[0mInference done 111/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0002 s/iter. Total: 0.1018 s/iter. ETA=0:01:30\n",
      "\u001b[32m[11/30 10:51:17 d2.evaluation.evaluator]: \u001b[0mInference done 160/1000. Dataloading: 0.0006 s/iter. Inference: 0.1011 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:01:25\n",
      "\u001b[32m[11/30 10:51:22 d2.evaluation.evaluator]: \u001b[0mInference done 209/1000. Dataloading: 0.0006 s/iter. Inference: 0.1011 s/iter. Eval: 0.0003 s/iter. Total: 0.1020 s/iter. ETA=0:01:20\n",
      "\u001b[32m[11/30 10:51:27 d2.evaluation.evaluator]: \u001b[0mInference done 258/1000. Dataloading: 0.0006 s/iter. Inference: 0.1012 s/iter. Eval: 0.0003 s/iter. Total: 0.1020 s/iter. ETA=0:01:15\n",
      "\u001b[32m[11/30 10:51:32 d2.evaluation.evaluator]: \u001b[0mInference done 307/1000. Dataloading: 0.0006 s/iter. Inference: 0.1012 s/iter. Eval: 0.0003 s/iter. Total: 0.1020 s/iter. ETA=0:01:10\n",
      "\u001b[32m[11/30 10:51:37 d2.evaluation.evaluator]: \u001b[0mInference done 356/1000. Dataloading: 0.0006 s/iter. Inference: 0.1012 s/iter. Eval: 0.0003 s/iter. Total: 0.1021 s/iter. ETA=0:01:05\n",
      "\u001b[32m[11/30 10:51:42 d2.evaluation.evaluator]: \u001b[0mInference done 404/1000. Dataloading: 0.0006 s/iter. Inference: 0.1013 s/iter. Eval: 0.0006 s/iter. Total: 0.1025 s/iter. ETA=0:01:01\n",
      "\u001b[32m[11/30 10:51:47 d2.evaluation.evaluator]: \u001b[0mInference done 453/1000. Dataloading: 0.0006 s/iter. Inference: 0.1013 s/iter. Eval: 0.0005 s/iter. Total: 0.1025 s/iter. ETA=0:00:56\n",
      "\u001b[32m[11/30 10:51:52 d2.evaluation.evaluator]: \u001b[0mInference done 502/1000. Dataloading: 0.0006 s/iter. Inference: 0.1014 s/iter. Eval: 0.0005 s/iter. Total: 0.1025 s/iter. ETA=0:00:51\n",
      "\u001b[32m[11/30 10:51:57 d2.evaluation.evaluator]: \u001b[0mInference done 551/1000. Dataloading: 0.0006 s/iter. Inference: 0.1014 s/iter. Eval: 0.0005 s/iter. Total: 0.1025 s/iter. ETA=0:00:46\n",
      "\u001b[32m[11/30 10:52:02 d2.evaluation.evaluator]: \u001b[0mInference done 600/1000. Dataloading: 0.0006 s/iter. Inference: 0.1014 s/iter. Eval: 0.0005 s/iter. Total: 0.1026 s/iter. ETA=0:00:41\n",
      "\u001b[32m[11/30 10:52:07 d2.evaluation.evaluator]: \u001b[0mInference done 649/1000. Dataloading: 0.0006 s/iter. Inference: 0.1015 s/iter. Eval: 0.0005 s/iter. Total: 0.1026 s/iter. ETA=0:00:36\n",
      "\u001b[32m[11/30 10:52:12 d2.evaluation.evaluator]: \u001b[0mInference done 698/1000. Dataloading: 0.0006 s/iter. Inference: 0.1015 s/iter. Eval: 0.0005 s/iter. Total: 0.1026 s/iter. ETA=0:00:30\n",
      "\u001b[32m[11/30 10:52:17 d2.evaluation.evaluator]: \u001b[0mInference done 747/1000. Dataloading: 0.0006 s/iter. Inference: 0.1015 s/iter. Eval: 0.0004 s/iter. Total: 0.1026 s/iter. ETA=0:00:25\n",
      "\u001b[32m[11/30 10:52:22 d2.evaluation.evaluator]: \u001b[0mInference done 796/1000. Dataloading: 0.0006 s/iter. Inference: 0.1015 s/iter. Eval: 0.0004 s/iter. Total: 0.1026 s/iter. ETA=0:00:20\n",
      "\u001b[32m[11/30 10:52:27 d2.evaluation.evaluator]: \u001b[0mInference done 844/1000. Dataloading: 0.0006 s/iter. Inference: 0.1016 s/iter. Eval: 0.0004 s/iter. Total: 0.1027 s/iter. ETA=0:00:16\n",
      "\u001b[32m[11/30 10:52:32 d2.evaluation.evaluator]: \u001b[0mInference done 893/1000. Dataloading: 0.0006 s/iter. Inference: 0.1017 s/iter. Eval: 0.0004 s/iter. Total: 0.1027 s/iter. ETA=0:00:10\n",
      "\u001b[32m[11/30 10:52:37 d2.evaluation.evaluator]: \u001b[0mInference done 942/1000. Dataloading: 0.0006 s/iter. Inference: 0.1017 s/iter. Eval: 0.0004 s/iter. Total: 0.1027 s/iter. ETA=0:00:05\n",
      "\u001b[32m[11/30 10:52:42 d2.evaluation.evaluator]: \u001b[0mInference done 991/1000. Dataloading: 0.0006 s/iter. Inference: 0.1017 s/iter. Eval: 0.0004 s/iter. Total: 0.1027 s/iter. ETA=0:00:00\n",
      "\u001b[32m[11/30 10:52:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:42.440289 (0.102955 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/30 10:52:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:41 (0.101680 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/30 10:52:44 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/30 10:52:44 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to SSLAD-2D/labeled/output/save\\inference\\coco_instances_results.json\n",
      "\u001b[32m[11/30 10:52:44 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/30 10:52:44 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.50 seconds.\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.13 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.129\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.099\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.050\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.106\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.113\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.201\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.062 | 12.920 | 2.184  | 2.471 | 4.835 | 9.896 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:52:45 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    | category   | AP     |\n",
      "|:-----------|:------|:-----------|:------|:-----------|:-------|\n",
      "| Pedestrian | 5.994 | Cyclist    | 5.413 | Car        | 17.493 |\n",
      "| Truck      | 1.148 | Tram       | 0.323 | Tricycle   | 0.000  |\n",
      "\u001b[32m[11/30 10:52:45 d2.engine.defaults]: \u001b[0mEvaluation results for soda10m_val_subset in csv format:\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/30 10:52:45 d2.evaluation.testing]: \u001b[0mcopypaste: 5.0620,12.9202,2.1840,2.4712,4.8348,9.8955\n",
      "Loss on Validation done 101/1000. 0.0000 s / img. ETA=0:01:34\n",
      "Loss on Validation done 201/1000. 0.0000 s / img. ETA=0:01:24\n",
      "Loss on Validation done 301/1000. 0.0000 s / img. ETA=0:01:14\n",
      "Loss on Validation done 401/1000. 0.0001 s / img. ETA=0:01:03\n",
      "Loss on Validation done 501/1000. 0.0000 s / img. ETA=0:00:52\n",
      "Loss on Validation done 601/1000. 0.0000 s / img. ETA=0:00:41\n",
      "Loss on Validation done 701/1000. 0.0000 s / img. ETA=0:00:31\n",
      "Loss on Validation done 801/1000. 0.0000 s / img. ETA=0:00:20\n",
      "Loss on Validation done 901/1000. 0.0000 s / img. ETA=0:00:10\n",
      "\u001b[32m[11/30 10:54:33 d2.utils.events]: \u001b[0m eta: 0:11:16  iter: 99  total_loss: 1.272  loss_cls: 0.4778  loss_box_reg: 0.6462  loss_rpn_cls: 0.03888  loss_rpn_loc: 0.08953  validation_loss: 1.134    time: 0.7903  last_time: 0.8121  data_time: 0.0012  last_data_time: 0.0011   lr: 0.00024975  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:54:47 d2.utils.events]: \u001b[0m eta: 0:10:58  iter: 119  total_loss: 1.226  loss_cls: 0.4887  loss_box_reg: 0.6193  loss_rpn_cls: 0.02762  loss_rpn_loc: 0.07179  validation_loss: 1.134    time: 0.7794  last_time: 0.8955  data_time: 0.0012  last_data_time: 0.0015   lr: 0.0002997  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:55:03 d2.utils.events]: \u001b[0m eta: 0:10:47  iter: 139  total_loss: 1.162  loss_cls: 0.4373  loss_box_reg: 0.6154  loss_rpn_cls: 0.03195  loss_rpn_loc: 0.07316  validation_loss: 1.134    time: 0.7782  last_time: 0.6256  data_time: 0.0013  last_data_time: 0.0015   lr: 0.00034965  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:55:18 d2.utils.events]: \u001b[0m eta: 0:10:32  iter: 159  total_loss: 1.01  loss_cls: 0.3536  loss_box_reg: 0.5277  loss_rpn_cls: 0.0274  loss_rpn_loc: 0.07689  validation_loss: 1.134    time: 0.7759  last_time: 0.6847  data_time: 0.0012  last_data_time: 0.0010   lr: 0.0003996  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:55:35 d2.utils.events]: \u001b[0m eta: 0:10:17  iter: 179  total_loss: 1.016  loss_cls: 0.3694  loss_box_reg: 0.5208  loss_rpn_cls: 0.02275  loss_rpn_loc: 0.08175  validation_loss: 1.134    time: 0.7856  last_time: 0.7679  data_time: 0.0014  last_data_time: 0.0012   lr: 0.00044955  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:55:54 d2.data.datasets.coco]: \u001b[0mLoaded 1000 images in COCO format from SSLAD-2D/labeled/annotations/instance_val_subset.json\n",
      "\u001b[32m[11/30 10:55:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/30 10:55:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/30 10:55:54 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/30 10:55:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.43 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/30 10:55:54 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[11/30 10:55:55 d2.evaluation.evaluator]: \u001b[0mStart inference on 1000 batches\n",
      "\u001b[32m[11/30 10:55:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/1000. Dataloading: 0.0004 s/iter. Inference: 0.1004 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:39\n",
      "\u001b[32m[11/30 10:56:03 d2.evaluation.evaluator]: \u001b[0mInference done 60/1000. Dataloading: 0.0006 s/iter. Inference: 0.1013 s/iter. Eval: 0.0003 s/iter. Total: 0.1023 s/iter. ETA=0:01:36\n",
      "\u001b[32m[11/30 10:56:08 d2.evaluation.evaluator]: \u001b[0mInference done 110/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:01:30\n",
      "\u001b[32m[11/30 10:56:13 d2.evaluation.evaluator]: \u001b[0mInference done 160/1000. Dataloading: 0.0006 s/iter. Inference: 0.1007 s/iter. Eval: 0.0003 s/iter. Total: 0.1016 s/iter. ETA=0:01:25\n",
      "\u001b[32m[11/30 10:56:18 d2.evaluation.evaluator]: \u001b[0mInference done 210/1000. Dataloading: 0.0006 s/iter. Inference: 0.1006 s/iter. Eval: 0.0003 s/iter. Total: 0.1015 s/iter. ETA=0:01:20\n",
      "\u001b[32m[11/30 10:56:23 d2.evaluation.evaluator]: \u001b[0mInference done 259/1000. Dataloading: 0.0006 s/iter. Inference: 0.1010 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:01:15\n",
      "\u001b[32m[11/30 10:56:28 d2.evaluation.evaluator]: \u001b[0mInference done 308/1000. Dataloading: 0.0006 s/iter. Inference: 0.1011 s/iter. Eval: 0.0003 s/iter. Total: 0.1020 s/iter. ETA=0:01:10\n",
      "\u001b[32m[11/30 10:56:33 d2.evaluation.evaluator]: \u001b[0mInference done 358/1000. Dataloading: 0.0006 s/iter. Inference: 0.1011 s/iter. Eval: 0.0003 s/iter. Total: 0.1020 s/iter. ETA=0:01:05\n",
      "\u001b[32m[11/30 10:56:38 d2.evaluation.evaluator]: \u001b[0mInference done 408/1000. Dataloading: 0.0006 s/iter. Inference: 0.1010 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:01:00\n",
      "\u001b[32m[11/30 10:56:43 d2.evaluation.evaluator]: \u001b[0mInference done 458/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:00:55\n",
      "\u001b[32m[11/30 10:56:48 d2.evaluation.evaluator]: \u001b[0mInference done 508/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:00:50\n",
      "\u001b[32m[11/30 10:56:54 d2.evaluation.evaluator]: \u001b[0mInference done 558/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:00:45\n",
      "\u001b[32m[11/30 10:56:59 d2.evaluation.evaluator]: \u001b[0mInference done 608/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:00:39\n",
      "\u001b[32m[11/30 10:57:04 d2.evaluation.evaluator]: \u001b[0mInference done 658/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:00:34\n",
      "\u001b[32m[11/30 10:57:09 d2.evaluation.evaluator]: \u001b[0mInference done 708/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:00:29\n",
      "\u001b[32m[11/30 10:57:14 d2.evaluation.evaluator]: \u001b[0mInference done 758/1000. Dataloading: 0.0006 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:00:24\n",
      "\u001b[32m[11/30 10:57:19 d2.evaluation.evaluator]: \u001b[0mInference done 808/1000. Dataloading: 0.0006 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:00:19\n",
      "\u001b[32m[11/30 10:57:24 d2.evaluation.evaluator]: \u001b[0mInference done 858/1000. Dataloading: 0.0006 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1017 s/iter. ETA=0:00:14\n",
      "\u001b[32m[11/30 10:57:29 d2.evaluation.evaluator]: \u001b[0mInference done 908/1000. Dataloading: 0.0006 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1017 s/iter. ETA=0:00:09\n",
      "\u001b[32m[11/30 10:57:34 d2.evaluation.evaluator]: \u001b[0mInference done 958/1000. Dataloading: 0.0006 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1017 s/iter. ETA=0:00:04\n",
      "\u001b[32m[11/30 10:57:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:41.410936 (0.101921 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/30 10:57:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:40 (0.100759 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/30 10:57:39 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/30 10:57:39 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to SSLAD-2D/labeled/output/save\\inference\\coco_instances_results.json\n",
      "\u001b[32m[11/30 10:57:39 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/30 10:57:39 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 10:57:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.30 seconds.\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.09 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.183\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.344\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.158\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.173\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.267\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.124\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.287\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.174\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.425\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 18.304 | 34.447 | 15.813 | 8.981 | 17.264 | 26.671 |\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| Pedestrian | 19.483 | Cyclist    | 24.201 | Car        | 47.827 |\n",
      "| Truck      | 13.174 | Tram       | 5.141  | Tricycle   | 0.000  |\n",
      "\u001b[32m[11/30 10:57:40 d2.engine.defaults]: \u001b[0mEvaluation results for soda10m_val_subset in csv format:\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/30 10:57:40 d2.evaluation.testing]: \u001b[0mcopypaste: 18.3042,34.4469,15.8133,8.9813,17.2639,26.6707\n",
      "Loss on Validation done 101/1000. 0.0000 s / img. ETA=0:01:30\n",
      "Loss on Validation done 201/1000. 0.0000 s / img. ETA=0:01:22\n",
      "Loss on Validation done 301/1000. 0.0000 s / img. ETA=0:01:11\n",
      "Loss on Validation done 401/1000. 0.0000 s / img. ETA=0:01:01\n",
      "Loss on Validation done 501/1000. 0.0000 s / img. ETA=0:00:51\n",
      "Loss on Validation done 601/1000. 0.0000 s / img. ETA=0:00:40\n",
      "Loss on Validation done 701/1000. 0.0000 s / img. ETA=0:00:30\n",
      "Loss on Validation done 801/1000. 0.0000 s / img. ETA=0:00:20\n",
      "Loss on Validation done 901/1000. 0.0000 s / img. ETA=0:00:10\n",
      "\u001b[32m[11/30 10:59:26 d2.utils.events]: \u001b[0m eta: 0:10:04  iter: 199  total_loss: 0.912  loss_cls: 0.3146  loss_box_reg: 0.4924  loss_rpn_cls: 0.02216  loss_rpn_loc: 0.06559  validation_loss: 0.8595    time: 0.8037  last_time: 0.7740  data_time: 0.0014  last_data_time: 0.0016   lr: 0.0004995  max_mem: 2607M\n",
      "\u001b[32m[11/30 10:59:48 d2.utils.events]: \u001b[0m eta: 0:09:52  iter: 219  total_loss: 0.897  loss_cls: 0.2922  loss_box_reg: 0.3875  loss_rpn_cls: 0.0255  loss_rpn_loc: 0.0703  validation_loss: 0.8595    time: 0.8301  last_time: 0.8067  data_time: 0.0014  last_data_time: 0.0013   lr: 0.00054945  max_mem: 2607M\n",
      "\u001b[32m[11/30 11:00:07 d2.utils.events]: \u001b[0m eta: 0:09:35  iter: 239  total_loss: 0.8882  loss_cls: 0.2934  loss_box_reg: 0.4211  loss_rpn_cls: 0.02817  loss_rpn_loc: 0.07268  validation_loss: 0.8595    time: 0.8417  last_time: 1.7382  data_time: 0.0013  last_data_time: 0.0010   lr: 0.0005994  max_mem: 2607M\n",
      "\u001b[32m[11/30 11:00:26 d2.utils.events]: \u001b[0m eta: 0:09:20  iter: 259  total_loss: 0.9066  loss_cls: 0.3031  loss_box_reg: 0.486  loss_rpn_cls: 0.02317  loss_rpn_loc: 0.08968  validation_loss: 0.8595    time: 0.8518  last_time: 1.7030  data_time: 0.0014  last_data_time: 0.0013   lr: 0.00064935  max_mem: 2607M\n",
      "\u001b[32m[11/30 11:00:45 d2.utils.events]: \u001b[0m eta: 0:09:06  iter: 279  total_loss: 0.8584  loss_cls: 0.3063  loss_box_reg: 0.4075  loss_rpn_cls: 0.02286  loss_rpn_loc: 0.09939  validation_loss: 0.8595    time: 0.8559  last_time: 0.7946  data_time: 0.0013  last_data_time: 0.0013   lr: 0.0006993  max_mem: 2607M\n",
      "\u001b[32m[11/30 11:01:00 d2.data.datasets.coco]: \u001b[0mLoaded 1000 images in COCO format from SSLAD-2D/labeled/annotations/instance_val_subset.json\n",
      "\u001b[32m[11/30 11:01:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/30 11:01:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/30 11:01:00 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/30 11:01:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.43 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/30 11:01:00 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[11/30 11:01:00 d2.evaluation.evaluator]: \u001b[0mStart inference on 1000 batches\n",
      "\u001b[32m[11/30 11:01:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/1000. Dataloading: 0.0005 s/iter. Inference: 0.1002 s/iter. Eval: 0.0003 s/iter. Total: 0.1010 s/iter. ETA=0:01:39\n",
      "\u001b[32m[11/30 11:01:08 d2.evaluation.evaluator]: \u001b[0mInference done 61/1000. Dataloading: 0.0006 s/iter. Inference: 0.1003 s/iter. Eval: 0.0003 s/iter. Total: 0.1011 s/iter. ETA=0:01:34\n",
      "\u001b[32m[11/30 11:01:13 d2.evaluation.evaluator]: \u001b[0mInference done 111/1000. Dataloading: 0.0006 s/iter. Inference: 0.1002 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:29\n",
      "\u001b[32m[11/30 11:01:18 d2.evaluation.evaluator]: \u001b[0mInference done 161/1000. Dataloading: 0.0006 s/iter. Inference: 0.1001 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:24\n",
      "\u001b[32m[11/30 11:01:23 d2.evaluation.evaluator]: \u001b[0mInference done 211/1000. Dataloading: 0.0006 s/iter. Inference: 0.1001 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:19\n",
      "\u001b[32m[11/30 11:01:28 d2.evaluation.evaluator]: \u001b[0mInference done 261/1000. Dataloading: 0.0006 s/iter. Inference: 0.1001 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:14\n",
      "\u001b[32m[11/30 11:01:33 d2.evaluation.evaluator]: \u001b[0mInference done 311/1000. Dataloading: 0.0006 s/iter. Inference: 0.1001 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:09\n",
      "\u001b[32m[11/30 11:01:38 d2.evaluation.evaluator]: \u001b[0mInference done 361/1000. Dataloading: 0.0006 s/iter. Inference: 0.1002 s/iter. Eval: 0.0002 s/iter. Total: 0.1010 s/iter. ETA=0:01:04\n",
      "\u001b[32m[11/30 11:01:44 d2.evaluation.evaluator]: \u001b[0mInference done 411/1000. Dataloading: 0.0006 s/iter. Inference: 0.1002 s/iter. Eval: 0.0002 s/iter. Total: 0.1011 s/iter. ETA=0:00:59\n",
      "\u001b[32m[11/30 11:01:49 d2.evaluation.evaluator]: \u001b[0mInference done 461/1000. Dataloading: 0.0006 s/iter. Inference: 0.1002 s/iter. Eval: 0.0002 s/iter. Total: 0.1011 s/iter. ETA=0:00:54\n",
      "\u001b[32m[11/30 11:01:54 d2.evaluation.evaluator]: \u001b[0mInference done 511/1000. Dataloading: 0.0006 s/iter. Inference: 0.1003 s/iter. Eval: 0.0002 s/iter. Total: 0.1012 s/iter. ETA=0:00:49\n",
      "\u001b[32m[11/30 11:01:59 d2.evaluation.evaluator]: \u001b[0mInference done 561/1000. Dataloading: 0.0006 s/iter. Inference: 0.1003 s/iter. Eval: 0.0002 s/iter. Total: 0.1012 s/iter. ETA=0:00:44\n",
      "\u001b[32m[11/30 11:02:04 d2.evaluation.evaluator]: \u001b[0mInference done 610/1000. Dataloading: 0.0006 s/iter. Inference: 0.1005 s/iter. Eval: 0.0003 s/iter. Total: 0.1014 s/iter. ETA=0:00:39\n",
      "\u001b[32m[11/30 11:02:09 d2.evaluation.evaluator]: \u001b[0mInference done 659/1000. Dataloading: 0.0006 s/iter. Inference: 0.1007 s/iter. Eval: 0.0003 s/iter. Total: 0.1016 s/iter. ETA=0:00:34\n",
      "\u001b[32m[11/30 11:02:14 d2.evaluation.evaluator]: \u001b[0mInference done 708/1000. Dataloading: 0.0006 s/iter. Inference: 0.1007 s/iter. Eval: 0.0003 s/iter. Total: 0.1017 s/iter. ETA=0:00:29\n",
      "\u001b[32m[11/30 11:02:19 d2.evaluation.evaluator]: \u001b[0mInference done 757/1000. Dataloading: 0.0006 s/iter. Inference: 0.1008 s/iter. Eval: 0.0003 s/iter. Total: 0.1017 s/iter. ETA=0:00:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/30 11:02:24 d2.evaluation.evaluator]: \u001b[0mInference done 806/1000. Dataloading: 0.0006 s/iter. Inference: 0.1009 s/iter. Eval: 0.0003 s/iter. Total: 0.1018 s/iter. ETA=0:00:19\n",
      "\u001b[32m[11/30 11:02:29 d2.evaluation.evaluator]: \u001b[0mInference done 854/1000. Dataloading: 0.0006 s/iter. Inference: 0.1010 s/iter. Eval: 0.0003 s/iter. Total: 0.1019 s/iter. ETA=0:00:14\n",
      "\u001b[32m[11/30 11:02:34 d2.evaluation.evaluator]: \u001b[0mInference done 902/1000. Dataloading: 0.0006 s/iter. Inference: 0.1012 s/iter. Eval: 0.0003 s/iter. Total: 0.1021 s/iter. ETA=0:00:10\n",
      "\u001b[32m[11/30 11:02:39 d2.evaluation.evaluator]: \u001b[0mInference done 951/1000. Dataloading: 0.0006 s/iter. Inference: 0.1012 s/iter. Eval: 0.0003 s/iter. Total: 0.1021 s/iter. ETA=0:00:05\n",
      "\u001b[32m[11/30 11:02:44 d2.evaluation.evaluator]: \u001b[0mInference done 1000/1000. Dataloading: 0.0006 s/iter. Inference: 0.1012 s/iter. Eval: 0.0003 s/iter. Total: 0.1021 s/iter. ETA=0:00:00\n",
      "\u001b[32m[11/30 11:02:44 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:41.874585 (0.102387 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/30 11:02:44 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:40 (0.101198 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to SSLAD-2D/labeled/output/save\\inference\\coco_instances_results.json\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.27 seconds.\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.08 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.226\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.425\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.205\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.099\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.196\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.324\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.143\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.364\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.196\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.336\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.469\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 22.559 | 42.525 | 20.530 | 9.889 | 19.556 | 32.405 |\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| Pedestrian | 18.270 | Cyclist    | 27.277 | Car        | 49.063 |\n",
      "| Truck      | 24.117 | Tram       | 16.629 | Tricycle   | 0.000  |\n",
      "\u001b[32m[11/30 11:02:45 d2.engine.defaults]: \u001b[0mEvaluation results for soda10m_val_subset in csv format:\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/30 11:02:45 d2.evaluation.testing]: \u001b[0mcopypaste: 22.5592,42.5252,20.5298,9.8888,19.5560,32.4053\n",
      "Loss on Validation done 101/1000. 0.0000 s / img. ETA=0:01:32\n",
      "Loss on Validation done 201/1000. 0.0000 s / img. ETA=0:01:22\n",
      "Loss on Validation done 301/1000. 0.0000 s / img. ETA=0:01:12\n",
      "Loss on Validation done 401/1000. 0.0000 s / img. ETA=0:01:02\n",
      "Loss on Validation done 501/1000. 0.0000 s / img. ETA=0:00:52\n",
      "Loss on Validation done 601/1000. 0.0000 s / img. ETA=0:00:41\n",
      "Loss on Validation done 701/1000. 0.0000 s / img. ETA=0:00:31\n",
      "Loss on Validation done 801/1000. 0.0000 s / img. ETA=0:00:20\n",
      "Loss on Validation done 901/1000. 0.0000 s / img. ETA=0:00:10\n",
      "\u001b[32m[11/30 11:04:34 d2.utils.events]: \u001b[0m eta: 0:08:51  iter: 299  total_loss: 0.7771  loss_cls: 0.2619  loss_box_reg: 0.4093  loss_rpn_cls: 0.02393  loss_rpn_loc: 0.08897  validation_loss: 0.8153    time: 0.8487  last_time: 0.5819  data_time: 0.0013  last_data_time: 0.0014   lr: 0.00074925  max_mem: 2607M\n",
      "\u001b[32m[11/30 11:04:50 d2.utils.events]: \u001b[0m eta: 0:08:36  iter: 319  total_loss: 0.9042  loss_cls: 0.3013  loss_box_reg: 0.4535  loss_rpn_cls: 0.01726  loss_rpn_loc: 0.108  validation_loss: 0.8153    time: 0.8468  last_time: 0.6616  data_time: 0.0013  last_data_time: 0.0011   lr: 0.0007992  max_mem: 2607M\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data.samplers import RandomSubsetTrainingSampler\n",
    "from detectron2.data import build_detection_test_loader   # the default mapper\n",
    "from detectron2.evaluation import inference_on_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "# Update the dataset names to the registered names\n",
    "cfg.DATASETS.TRAIN = (\"soda10m_train\",)\n",
    "cfg.DATASETS.TEST = (\"soda10m_val_subset\",)  # or an empty tuple if you don't have a validation set\n",
    "\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "#cfg.MODEL.WEIGHTS = r\"C:\\Users\\melmourabitagharbi\\detectron2\\detectron2\\checkpoint\\faster_rcnn_R_101_FPN_3x\\model_final_f6e8b1.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = []  # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6  # Number of classes in SODA10m\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = 100\n",
    "\n",
    "\n",
    "num_gpu = 1\n",
    "bs = (num_gpu * 2)\n",
    "cfg.SOLVER.BASE_LR = 0.02 * bs / 16  # pick a good LR\n",
    "#cfg.SOLVER.BASE_LR = 0.01  # pick a good LR\n",
    "\n",
    "# Before starting your training or inference\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'\n",
    "\n",
    "# Setup the output directory\n",
    "cfg.OUTPUT_DIR = r\"SSLAD-2D/labeled/output/save\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Create the trainer and start training\n",
    "#trainer = DefaultTrainer(cfg) #Training without evaluation.\n",
    "trainer = MyTrainer(cfg) #Training with evaluation\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03506601",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sampler = RandomSubsetTrainingSampler(5000, 0.2)\n",
    "val_loader = build_detection_test_loader(cfg,\"soda10m_val\", sampler=subset_sampler)\n",
    "\n",
    "inference_on_dataset(trainer.model, val_loader,MyTrainer.get_evaluator(cfg, \"soda10m_val\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a483e6e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c714ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "# Set up the configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"soda10m_train\",)\n",
    "cfg.DATASETS.TEST = (\"soda10m_val\",)  # Provide the tuple even if no validation set is available\n",
    "cfg.MODEL.WEIGHTS = r\"SSLAD-2D\\labeled\\output\\save\\250k\\model_final.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a testing threshold\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 100  # Adjust as necessary\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6  # Set the number of classes\n",
    "\n",
    "# Build the test loader\n",
    "test_loader = build_detection_test_loader(cfg, \"soda10m_val\")\n",
    "\n",
    "# Create a COCOEvaluator instance\n",
    "evaluator = COCOEvaluator(\"soda10m_val\", cfg, False, output_dir=r\"SSLAD-2D\\labeled\\output\\save\")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "# Perform the evaluation\n",
    "eval_results = inference_on_dataset(trainer.model, test_loader, evaluator)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8d07f",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e958a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Load the validation dataset\n",
    "dataset_dicts = DatasetCatalog.get(\"soda10m_val\")\n",
    "metadata = MetadataCatalog.get(\"soda10m_val\")\n",
    "\n",
    "# Assuming you want to visualize the ground truth for the first image in the dataset\n",
    "img_dict = dataset_dicts[0]  # or any other index\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(img_dict[\"file_name\"])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw ground truth annotations on the image\n",
    "v_gt = Visualizer(image, metadata, scale=1.2)\n",
    "out_gt = v_gt.draw_dataset_dict(img_dict)\n",
    "im_gt = out_gt.get_image()\n",
    "\n",
    "# Now `im_gt` contains the image with ground truth annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e545bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36434a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa776f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and model loading for Faster R-CNN\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = r\"SSLAD-2D\\labeled\\output\\save250k\\model_final.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6  # Set this to the number of classes you have, same as during training\n",
    "\n",
    "\n",
    "# Create predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Make prediction\n",
    "outputs = predictor(image)\n",
    "\n",
    "# We use Visualizer to draw the predictions on the image.\n",
    "v = Visualizer(image[:, :, ::-1], metadata, scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "\n",
    "# OpenCV uses BGR color format, and matplotlib uses RGB.\n",
    "# So, we need to convert the image from BGR to RGB format.\n",
    "im_pred = cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display predictions and ground truth side by side using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(im_pred)\n",
    "plt.title('Model Predictions')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(im_gt)\n",
    "plt.title('Ground Truth')\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the figure to a file\n",
    "plt.savefig(\"comparison_plot.png\", bbox_inches='tight', dpi=300)\n",
    "# Optionally, you can still display the plot if you want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50486276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
